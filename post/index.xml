<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Posts | Xiaogang Jia</title>
    <link>https://xiaogangjia.github.io/post/</link>
      <atom:link href="https://xiaogangjia.github.io/post/index.xml" rel="self" type="application/rss+xml" />
    <description>Posts</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><lastBuildDate>Mon, 01 Jun 2020 00:00:00 +0000</lastBuildDate>
    <image>
      <url>https://xiaogangjia.github.io/images/icon_hu0b7a4cb9992c9ac0e91bd28ffd38dd00_9727_512x512_fill_lanczos_center_2.png</url>
      <title>Posts</title>
      <link>https://xiaogangjia.github.io/post/</link>
    </image>
    
    <item>
      <title>INTERPRET trajectory prediction</title>
      <link>https://xiaogangjia.github.io/post/sgan/</link>
      <pubDate>Mon, 01 Jun 2020 00:00:00 +0000</pubDate>
      <guid>https://xiaogangjia.github.io/post/sgan/</guid>
      <description>&lt;h2 id=&#34;interaction-challenge-in-cvpr-2020-workshop-scalability-in-autonomous-driving&#34;&gt;Interaction Challenge in CVPR 2020 workshop, &amp;ldquo;Scalability in Autonomous Driving&amp;rdquo;&lt;/h2&gt;
&lt;p&gt;
&lt;a href=&#34;https://sites.google.com/view/cvpr20-scalability/competitions&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Workshop&lt;/a&gt;

&lt;a href=&#34;http://challenge.interaction-dataset.com/leader-board&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Leaderboard&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;I ranked &lt;code&gt;5th&lt;/code&gt; in the regular track and ranked &lt;code&gt;2nd&lt;/code&gt; for metric of MON.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Object Detection in Aerial Images</title>
      <link>https://xiaogangjia.github.io/post/visdrone/</link>
      <pubDate>Fri, 01 May 2020 00:00:00 +0000</pubDate>
      <guid>https://xiaogangjia.github.io/post/visdrone/</guid>
      <description>&lt;h2 id=&#34;visdrone-challenge-in-eccv-2020-workshop&#34;&gt;VisDrone Challenge in ECCV 2020 workshop&lt;/h2&gt;
&lt;p&gt;
&lt;a href=&#34;http://aiskyeye.com/leaderboard/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Leaderboard&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;I achieved an AP of 22.72% in the object detection challenge.&lt;/p&gt;
&lt;h2 id=&#34;details&#34;&gt;Details&lt;/h2&gt;
&lt;p&gt;Here I proposed a &lt;code&gt;point-based&lt;/code&gt; network to localize objects in a low-resolution feature map. To save computational costs, I used &lt;code&gt;mobilenetv3&lt;/code&gt; to be the backbone. Then &lt;code&gt;K-Means&lt;/code&gt; is used as a post-processing method to generate high-quality clusters.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;./cluster.jpg&#34; alt=&#34;clusters&#34;&gt;&lt;/p&gt;
&lt;p&gt;Both original images and cropped images are processed by the detector. The detector is based on &lt;code&gt;CenterNet&lt;/code&gt; and &lt;code&gt;Hourglass-104&lt;/code&gt;. The model is initialized with weights pre-trained on MS COCO and I trained it for 30 epochs. In the end, all the predicted bounding boxes are merged by standard &lt;code&gt;NMS&lt;/code&gt;.&lt;/p&gt;
&lt;h2 id=&#34;test-on-challenge-set&#34;&gt;Test on challenge set&lt;/h2&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;AP&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;22.72%&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h2 id=&#34;test-on-validation-set&#34;&gt;Test on validation set&lt;/h2&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;AP&lt;/th&gt;
&lt;th&gt;AP50&lt;/th&gt;
&lt;th&gt;AP75&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;31.06%&lt;/td&gt;
&lt;td&gt;54.69%&lt;/td&gt;
&lt;td&gt;29.09%&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
</description>
    </item>
    
    <item>
      <title>Defect Detection for Sanitary Ceramics</title>
      <link>https://xiaogangjia.github.io/post/defect/</link>
      <pubDate>Wed, 05 Feb 2020 00:00:00 +0000</pubDate>
      <guid>https://xiaogangjia.github.io/post/defect/</guid>
      <description>&lt;h2 id=&#34;overview-of-the-detector&#34;&gt;Overview of the detector&lt;/h2&gt;
&lt;p&gt;&lt;img src=&#34;./net.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;h2 id=&#34;backbone&#34;&gt;Backbone&lt;/h2&gt;
&lt;p&gt;This work is based on &lt;code&gt;CenterNet&lt;/code&gt;. I simplified the original &lt;code&gt;ResNet18&lt;/code&gt; and the volume of the final model is 34.3MB. It also realized full-resolution prediction which is beneficial for point-based detection.
&lt;img src=&#34;./backbone.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;h2 id=&#34;adaptive-feature-fusion&#34;&gt;Adaptive feature fusion&lt;/h2&gt;
&lt;p&gt;I introduced an extra branch with shallow layers to strengthen the feature representation. Then an &lt;code&gt;adaptive feature fusion&lt;/code&gt; method is used to fuse main features with low-level features. This module was realized by a 1*1 conv layer and a softmax function, so that the network was able to learn the weights between deep-layer features and shallow-layer features.&lt;/p&gt;
&lt;h2 id=&#34;experiment-on-the-validation-set&#34;&gt;Experiment on the validation set&lt;/h2&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;AP50&lt;/th&gt;
&lt;th&gt;Volume&lt;/th&gt;
&lt;th&gt;FPS&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;96.16%&lt;/td&gt;
&lt;td&gt;34.3MB&lt;/td&gt;
&lt;td&gt;30&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
</description>
    </item>
    
    <item>
      <title>Intelligent Monitoring System for Belt Conveyor</title>
      <link>https://xiaogangjia.github.io/post/yingyingying/</link>
      <pubDate>Fri, 30 Aug 2019 00:00:00 +0000</pubDate>
      <guid>https://xiaogangjia.github.io/post/yingyingying/</guid>
      <description>&lt;h2 id=&#34;tasks&#34;&gt;Tasks&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;Original &lt;code&gt;YOLOv3&lt;/code&gt; with official pre-trained weights is used to detect workers. &lt;code&gt;YOLOv3&lt;/code&gt; with modified anchors is used to detect smoke and fire.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;Mask-RCNN&lt;/code&gt; is used to segment the coal region in a pre-defined area. I also find that a simple &lt;code&gt;U-Net&lt;/code&gt; can perform very well.&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;demos&#34;&gt;Demos&lt;/h2&gt;
&lt;p&gt;&lt;img src=&#34;./seg.gif&#34; alt=&#34;&#34;&gt;
&lt;img src=&#34;./smoke.gif&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;h2 id=&#34;ps&#34;&gt;P.S.&lt;/h2&gt;
&lt;p&gt;For some reason, all models here are not reported with metrics.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>License Plate Recognition in Natural Scenes</title>
      <link>https://xiaogangjia.github.io/post/lp/</link>
      <pubDate>Mon, 01 Jul 2019 00:00:00 +0000</pubDate>
      <guid>https://xiaogangjia.github.io/post/lp/</guid>
      <description>&lt;h2 id=&#34;pipeline&#34;&gt;Pipeline&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;Original &lt;code&gt;YOLOv2&lt;/code&gt; with VOC pre-trained weights is used to detect all cars in the image.&lt;/li&gt;
&lt;li&gt;A novel &lt;code&gt;CNN&lt;/code&gt; is proposed to predict a set of &lt;code&gt;affine transformation&lt;/code&gt; parameters which are used to extract the area of license plates. Then a &lt;code&gt;perspective transformation&lt;/code&gt; is applied to rectify the distorted license plates.&lt;/li&gt;
&lt;li&gt;Simplify the backbone of YOLOv2 to detect characters in the rectified regions.&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;experiment&#34;&gt;Experiment&lt;/h2&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Total&lt;/th&gt;
&lt;th&gt;TP&lt;/th&gt;
&lt;th&gt;Accuracy&lt;/th&gt;
&lt;th&gt;FPS&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;200&lt;/td&gt;
&lt;td&gt;189&lt;/td&gt;
&lt;td&gt;94.5%&lt;/td&gt;
&lt;td&gt;10&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h2 id=&#34;examples-for-dataset&#34;&gt;Examples for dataset&lt;/h2&gt;
&lt;p&gt;&lt;img src=&#34;./car.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;h2 id=&#34;authentic-lps-and-synthesized-lps&#34;&gt;authentic LPs and synthesized LPs&lt;/h2&gt;
&lt;p&gt;&lt;img src=&#34;./true.png&#34; alt=&#34;true&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;./fake.png&#34; alt=&#34;fake&#34;&gt;&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
